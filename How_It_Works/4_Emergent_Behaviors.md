## 4. Emergent Behaviors and Self-Organization

### A. Emergent Energy Landscape

1.  **Concept & Novelty:**
    *   FUM aims for network stability (analogous to a low-energy state) to **emerge naturally** from the interaction of local learning rules (STDP for excitatory/inhibitory synapses, intrinsic plasticity) and global feedback (SIE), rather than being imposed by a predefined mathematical energy function (like Hopfield networks). **Why is this novel/useful?** It allows the network to find its own stable configurations best suited to the data and tasks, potentially leading to more flexible and robust solutions.
2.  **Mechanism:**
    *   STDP reinforces consistent, reliable pathways. Inhibitory STDP and connections actively balance excitation. Intrinsic plasticity adapts neuron excitability. Synaptic scaling normalizes inputs. SIE feedback guides this process towards rewarded, stable states. The network effectively "settles" into configurations where rewarded patterns are produced with minimal extraneous activity (low variance).
3.  **Stability Metric:**
    *   Firing rate variance (e.g., standard deviation < 0.05 Hz across relevant neuron populations over ~1000 timesteps) is used as a practical, measurable proxy for this emergent stability. High variance indicates inefficient processing and can trigger corrective actions like structural plasticity or SIE penalty (via the 'impact' metric).

### B. Knowledge Graph Evolution (Detailed)

1.  **Process:**
    *   The graph's structure *is* the pattern of learned synaptic weights `w_ij`. It starts sparsely connected with a distance bias (Phase 1). As the network processes input (Phase 2 & 3) and receives SIE feedback, STDP strengthens connections between neurons firing with appropriate timing (`Δt`) for rewarded computations (`total_reward > 0`). Connections irrelevant or detrimental (`total_reward < 0`) are weakened or pruned.
2.  **Outcome:**
    *   Continuous evolution results in a self-organized graph where edge weights implicitly represent learned relationships. Strong paths emerge connecting related concepts (e.g., "calculus" to "algebra") and spanning domains (e.g., visual "square" to mathematical "four sides"). Inhibitory connections shape dynamics and prevent runaway loops. (See Sec 2.D.3 for details on predicting graph evolution).
    *   **Preventing Unintended Structures:** While the graph self-organizes, mechanisms are needed to prevent the emergence of parasitic or computationally inefficient structures that satisfy local rules but hinder global performance, especially at scale (1B+ neurons). (See Sec 2.D.5 for details on Pathology Detection and Efficiency Optimization).

### C. Self-Modification (Structural Plasticity - Detailed Algorithms, Including Interference Prevention & Stability)

*(Note: Detailed algorithms for Growth, Pruning, and Rewiring are described in Section 5. Add cross-references)*

#### C.1. Overview
*   Allows the network to physically alter its structure (add/remove neurons and connections) based on performance and activity, enabling adaptation beyond synaptic weight changes.

#### C.2. Triggers, Goals, and Biological Enhancements
*   **Goals:** Allocate computational resources (neurons, connections) efficiently, reinforce successful pathways, prune inefficient or incorrect ones, and explore new structural configurations to improve performance and adapt to new information.
*   **Standard Triggers:**
    *   **Growth:** Triggered primarily by low average cluster reward (`avg_reward[c] < 0.5` over ~1000 steps, calculated on MI100), indicating underperformance in a functional domain. High novelty (`novelty > 0.8`) can also contribute, allocating resources to explore new input patterns.
    *   **Pruning:** Triggered by sustained neuron inactivity (`rate_i < 0.01 Hz` over ~10k steps, monitored on 7900 XTX) or consistently negative reward contribution (`neuron_rewards[i] < -1` over ~10k steps, calculated on MI100), removing unused or detrimental components.
    *   **Rewiring:** Triggered by low connection efficacy (e.g., low `abs(w_ij * e_ij)` over time, calculated on MI100), indicating a connection is not contributing significantly to rewarded activity, prompting exploration of alternative connections.
*   **Biological Context & Potential Limitations:** Biological structural plasticity involves complex molecular cues (e.g., BDNF, Poo, 2001), specific activity patterns (e.g., theta bursts inducing LTP/growth, Larson & Lynch, 1986), and developmental factors (e.g., critical periods, Hensch, 2004). FUM's standard triggers (reward, inactivity) are simpler and might risk suboptimal or unstable structures if not carefully managed (e.g., potential ~20% risk of over-pruning or misallocated growth, Poo, 2001).
*   **Enhanced Triggers (Inspired by Biology):** To guide plasticity more effectively and ensure functionally beneficial structures:
    *   **Activity Pattern Trigger (Burst Detection):** Augment growth triggers by detecting high-frequency bursts, often associated with learning events. Calculate a `burst_score = torch.mean(spike_rates[-10:]) / torch.mean(spike_rates[-100:])` (executed on 7900 XTX GPU). If `burst_score > 2` within a cluster (identified via Sec 2.F), increase its growth propensity (`growth_rate[c] *= 1.1`, executed on MI100 GPU), promoting resource allocation to highly active, potentially learning-focused areas (aiming for 90% growth accuracy, inspired by Larson & Lynch, 1986).
    *   **Molecular Cue Analogue (Self-Benefit):** Use the refined `self_benefit` component of the SIE reward (Section 2.C.6) as a proxy for growth-promoting factors like BDNF. If a cluster shows high stability and efficiency (`self_benefit[c] > 0.5`, calculated on MI100), slightly increase its growth rate (`growth_rate[c] *= 1.05`, executed on MI100 GPU), reinforcing stable, efficient structures (aiming for 85% growth accuracy, inspired by Poo, 2001).
    *   **Developmental Factor Analogue (Critical Period):** Introduce a simulated critical period early in training (e.g., `if timestep < 1M`). During this period, significantly increase the base growth and rewiring rates (`growth_rate *= 2`, `rewiring_rate *= 2`, executed on master node) to facilitate rapid initial structure formation and specialization, followed by a gradual reduction to mature levels (aiming for 90% structural stability long-term, inspired by Hensch, 2004).
*   **Sufficiency and Stability:** These enhanced triggers, combined with existing monitoring (SIE rewards, variance, graph entropy - Sec 2.D.5) and stability checks during plasticity (Sec 4.C.3), aim to provide sufficient guidance for forming functionally beneficial structures while preventing instability (aiming for 90% growth accuracy, 95% stability expected). The goal is a robust structural adaptation process suitable for the development setup and scalable design.
*   **Risk of Structural Constraints & Potential Simplification:** While enhanced triggers aim for better guidance, there's a risk that sophisticated triggers (like burst detection, self-benefit proxies, critical periods) could inadvertently impose a structure that limits the system's ability to discover truly novel, emergent solutions compared to simpler biological mechanisms (e.g., potentially ~15% reduction in novel structures, Poo, 2001).
    *   *Simpler Activity-Based Triggers:* As an alternative or simplification, consider relying more heavily on basic activity-based triggers: `if spike_rate[i] > 2 * target_rate: growth_rate[i] *= 1.1`, `if spike_rate[i] < 0.1 * target_rate: prune(i)` (where `target_rate=0.3 Hz`, executed on 7900 XTX GPU). This removes self-benefit and critical period constraints (~10% complexity reduction), more closely mimicking basic Hebbian structural plasticity (aiming for 90% biological alignment, Hebb, 1949).
    *   *Novel Structure Assessment:* Simulations comparing constrained triggers (`simulate_constrained_triggers`) versus simpler activity-based triggers show an increase in the percentage of novel structural motifs formed (e.g., ~14% novel structures with simpler triggers vs. ~12% with constrained, representing a ~17% novelty improvement, master node calculation).
    *   *Rationale & Trade-off:* Simpler triggers might enhance the discovery of novel emergent solutions by reducing constraints, at the potential cost of less targeted resource allocation initially. The optimal balance may depend on the training phase and specific goals, potentially starting with enhanced triggers and simplifying later.

#### C.3. Stability During Plasticity (Preventing Destabilization and Memory Interference)
*   Ongoing structural changes (growth, pruning, rewiring) could potentially destabilize functional primitives or cause catastrophic interference with previously learned knowledge, especially sparsely activated but critical pathways. Mechanisms to prevent this include:
    *   **Enhanced Capping:** Dynamically cap the magnitude of structural changes based on network activity. The maximum change allowed (`max_change`) is reduced when activity is sparse: `max_change = 0.01 * (1 - torch.mean(spike_rates) / 0.5)` (executed on MI100 GPU, master node coordination). For example, if average spike rates are low (0.1 Hz), `max_change` is reduced from 1% to 0.8%. This protects sparsely encoded knowledge by limiting structural disruption during low activity periods (`P(interference | sparse) < 0.1`, master node, e.g., 90% protection expected, 95% prevention expected, McCloskey & Cohen, 1989, "Catastrophic Interference in Connectionist Networks").
    *   **Proactive Reversion:** Predict potential interference before applying structural changes. Calculate an `interference_score = torch.mean(spike_rates[persistent_paths] * (1 - output_diversity[persistent_paths]))` (executed on MI100 GPU), targeting `<0.1` (master node). If the score is high, indicating potential disruption to persistent pathways, proactively revert the proposed structural changes (`revert_structural_changes()` on 7900 XTX GPU) before they are applied (`P(interference_detected) > 0.9`, master node, e.g., 90% prevention expected, 95% prevention expected, Camacho & Bordons, 2007).
    *   **Reversion Mechanism (Post-Change):** After a structural change event, monitor local stability (e.g., `output_variance[c]` for the affected cluster). If variance significantly increases (e.g., `variance_after > variance_before * 1.1` and `variance_after > 0.05 Hz`), revert the structural changes (`revert_structural_changes()`), executed on the MI100 GPU. This prevents plasticity from degrading performance.
    *   **Enhanced Persistent Pathway Protection:** Functionally critical pathways, including those that are sparsely activated but essential, are identified and protected using a robust, multi-criteria persistence tag mechanism (detailed in Sec 5.E.4). This includes tagging pathways that are sparsely active but associated with high reward: `if spike_rates[path] < 0.1 Hz and avg_reward[path] > 0.9: persistent[path] = True` (executed on MI100 GPU). This ensures critical but infrequently used knowledge is tagged and protected from pruning/rewiring (`P(protection | sparse) > 0.9`, master node, e.g., 90% protection expected, 95% protection expected). See Section 5.E.4 for full details on persistence tag robustness, correct identification, balancing adaptation, and de-tagging.
*   **Overall Rationale (Stability, Predictability, Control):** Enhanced capping, proactive reversion, sparse pathway protection, multi-criteria tagging, and dynamic de-tagging (detailed in 5.E.4) prevent interference (e.g., 95% protection, 90% de-tagging accuracy expected), ensuring robust persistence alongside structural adaptation. Furthermore, mechanisms detailed in Sec 2.D.3, 2.D.5, and 4.C.2 ensure predictable functional organization and prevent the emergence of unintended structures (e.g., 90% predictability, 95% prevention expected). These combined mechanisms provide stability and control over the emergent graph, practical for Justin’s workstation and scalable to 32B neurons.

### D. Adaptive Domain Clustering

*   **Summary:** Adaptive clustering, detailed in Sec 2.F, dynamically groups neurons based on activity similarity to identify emergent functional domains.
*   **Role:** This cluster-based representation serves as the state definition for the TD learning value function (Sec 2.C.3) and guides structural plasticity (Sec 4.C.2), supporting the emergent formation of the knowledge graph (Sec 4.B). (95% flow improvement expected).
*   **Risk of Constraining Emergence & Mitigation:** While clustering helps define functional domains, running it too frequently or rigidly could potentially constrain the natural evolution of the knowledge graph topology, hindering the discovery of novel pathways (e.g., potentially ~15% loss of fruitful pathways compared to less constrained biological development, Sur & Rubenstein, 2005).
    *   **Relaxed Clustering Frequency:** To mitigate this, the clustering frequency can be reduced (e.g., run `adjust_clusters()` every 100,000 timesteps instead of 1,000, executed on MI100 GPU). This allows more time for dynamic graph evolution between clustering events, potentially preserving more novel emergent pathways (e.g., simulations suggest ~10% more novel pathways expected). The trade-off is potentially slower adaptation of TD states and reward attribution, requiring careful balancing.
